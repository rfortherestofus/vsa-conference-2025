---
title: "VSA 2025 R Session"
author: "Brian Slattery & David Keyes"
format: 
  dashboard:
    scrolling: true
execute: 
  warning: false
  echo: false
  message: false
  output: true
---

```{r packages}
# These are all of the packages (collections of R functions) that are used to create this report
# These are separately installed with install.packages() & need to be enabled here using the library() function

# The tidyverse package (actually, a collection of packages) is a broad set of functions for organizing, analyzing, and visualizing data
# Tidyverse packages assume that your data is in a particular format-- but this allows them to work seamlessly together
# This will often provide the vast majority of additional capability that you'll need for any project
library(tidyverse)

# This allows you to smoothly authenticate with Google and download data from Google Sheets
library(googlesheets4)

# This provides a clean_names function that makes variables easier to work with from imported data
library(janitor)

# This has specialized functions for creating & manipulating tables for dataviz/reports
library(gt)

# This helps to deal with date & time data
library(lubridate)

# This is are packages for handling & visualizing geographical data
library(sf)
library(tigris)
library(tidygeocoder)

# This provides easy categorization for free response gender data
# > If you're doing this yourself, it has a slightly different installation than the other packages
# devtools::install_github("ropensci/gendercoder")
library(gendercoder)
```

```{r import}
# The first step is to import the survey responses from Google Sheets
# Here's the link to add new responses to the survey: https://forms.gle/1BRREmqWbVB6x8nm9

# Authenticating with Google is necessary before downloading data from Sheets
# > If you're doing this yourself later, you can instead uncomment the following line and add your own email address
# gs4_auth("YOUR_EMAIL_HERE")
gs4_auth(Sys.getenv("GOOGLE_SHEETS_EMAIL"))

# This gets the responses from the Google Sheet, and saves it with the name "responses"
# Doing this later on your own, you could replace the spreadsheet URL with one you created
# But you would need to have the same questions as the survey linked above, for the following sections of code to work
responses <- read_sheet(
  "https://docs.google.com/spreadsheets/d/1fL0_iM95kR2YU6om8ZE6yk_2ufp9W4RdR35gVxvi9Q8/"
)

# This cleans up the variable names that came in from Sheets, to make them easier to work with later
responses <- clean_names(responses)
```

## Row {height=10%}

### Column {width=50%}

```{r experience}
#| output: false

# First let's look at the question about previous experience with R

# I want to know how many people responded with each of the possible options
# So I group_by() the name of the column that I want to look at, and then count()
responses |>
  group_by(how_much_experience_do_you_have_with_r) |>
  count()

# Another way to do this is to use summarize() instead of count()
# Summarizing allows me to do other things besides counting, like getting the max, average, etc
# The n() function here, used inside sum(), gets me the number of responses
# And I can choose what the resulting column is called (in this case, "total")-- count() names it "n" by default
responses |>
  group_by(how_much_experience_do_you_have_with_r) |>
  summarize(total = sum(n()))

# However, the options for the experience question are listed in alphabetical order, what if I want to see them a different way?
# I can view them in descending order of number of responses, using arrange() and desc()
# (The ungroup() function is necessary if you do other steps after counting or summarizing)
responses |>
  group_by(how_much_experience_do_you_have_with_r) |>
  count() |>
  ungroup() |>
  arrange(desc(n))

# But my options are themselves meant to be in an order (none, slight, moderate, a lot)
# So I can format this variable into a "factor", which is a type of column that has ordering information (called "levels") saved in it
# To do this, I need to change my experience column with mutate()
# (I also first rename the column to a shorter name so it's easier to read)
responses |>
  rename(experience = how_much_experience_do_you_have_with_r) |>
  group_by(experience) |>
  count() |>
  ungroup() |>
  mutate(
    experience = fct(
      experience,
      levels = c(
        "None at all",
        "Slight experience",
        "Moderate experience",
        "A lot of experience"
      )
    )
  ) |>
  arrange(experience)

# You can also add a new column using mutate, that shows the percent of responses in each category
responses |>
  rename(experience = how_much_experience_do_you_have_with_r) |>
  group_by(experience) |>
  count() |>
  ungroup() |>
  mutate(percent = n / n()) |>
  mutate(
    experience = fct(
      experience,
      levels = c(
        "None at all",
        "Slight experience",
        "Moderate experience",
        "A lot of experience"
      )
    )
  ) |>
  arrange(experience)
```

```{r experience-table}
#| title: R Experience

# Building on the final example above, this is rendered into a table with gt()
# The names for the columns are adjusted with cols_label(), a function that's part of the gt package
# Additional steps do some formatting
# - Making the percent column to look more presentable, rather than a raw number
# - Increasing the font size
# - Setting the table width to 100%, so it fills the dashboard area it's in
responses |>
  rename(experience = how_much_experience_do_you_have_with_r) |>
  group_by(experience) |>
  count() |>
  ungroup() |>
  mutate(percent = n / n()) |>
  mutate(
    experience = fct(
      experience,
      levels = c(
        "None at all",
        "Slight experience",
        "Moderate experience",
        "A lot of experience"
      )
    )
  ) |>
  arrange(experience) |>
  mutate(percent = scales::label_percent(1)(percent)) |>
  gt() |>
  cols_label(experience = "Level of R Experience", n = "#", percent = "%") |>
  tab_options(table.font.size = 24, table.width = pct(100))
```


### Column {width=50%}

```{r dates}
#| output: false
# It's common to have to clean up date & time data when it's messy or nonstandard
# For example, Excel or Google Sheets can have a hard time taking a date in text format, and turning it into a usable "date" format
# R is very good at cleaning up messy date data-- here's the birthday data from the survey that is intentionally messed up a bit
responses |>
  select(bad_birthday)

# The parse_date_time() function in the lubridate package can quickly transform text dates into a useable format
# Like above we add a new column by mutating an existing column, using bad_birthday to create good_birthday with parse_date_time()
responses |>
  select(bad_birthday) |>
  mutate(good_birthday = parse_date_time(bad_birthday, orders = "mdy"))

# This allows us to do things like easily get the month from the date using month()
# Which can be used to highlight which months of the year have the most birthdays from our session attendees
# (This uses same grouping/counting steps as above)
responses |>
  select(bad_birthday) |>
  mutate(good_birthday = parse_date_time(bad_birthday, orders = "mdy")) |>
  mutate(month = month(good_birthday, label = TRUE)) |>
  group_by(month) |>
  count() |>
  arrange(desc(n))
```

```{r dates-chart}
#| title: Birthday Months
# Let's show these more frequent birthday months in a visual
# Continuing from the previous section, with some new steps to add in any months that might be missing (using a joining function)
# The bits after ggplot() are what create this particular visualization & style it visually
# (ggplot is very powerful but also very complex, and we aren't covering it in detail here-- but you can learn more from our linked resources!)
responses |>
  select(bad_birthday) |>
  mutate(good_birthday = parse_date_time(bad_birthday, orders = "mdy")) |>
  mutate(month = month(good_birthday, label = TRUE)) |>
  group_by(month) |>
  count() |>
  ungroup() |>
  right_join(as_tibble(month.abb) |> rename(month = value)) |>
  mutate(month = fct(month, levels = month.abb)) |>
  mutate(n = case_when(is.na(n) ~ 0, .default = n)) |>
  ggplot(aes(x = month, y = n)) +
  geom_col(fill = "#007765") +
  theme_minimal(base_size = 20) +
  theme(
    axis.title = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  scale_fill_gradient(low = "white", high = "#007765")
```

## Row

```{r filtering}
#| output: false
# R is great for filtering data in all kinds of ways
# For example, let's filter out previous responses, so we can see how many new responses we've gotten
# Google Sheets adds a timestamp to every new response, so we can use that to compare
# R has a built-in function, today() that tells you what day it is
# So if the timestamp is greater than today() (since it starts at midnight), then the response came in today
responses |>
  filter(timestamp > today())

# We'll use this to get the number of new, previous, and total responses, to show in highlighted boxes below
```

```{r new-resp}
#| content: valuebox
#| title: New responses
#| icon: pencil
#| color: "#6AA341"
responses |>
  filter(timestamp > today()) |>
  nrow()
```

```{r old-resp}
#| content: valuebox
#| title: Old responses
#| icon: clock
#| color: "#E4E192"
responses |>
  filter(timestamp < today()) |>
  nrow()
```

```{r total-resp}
#| content: valuebox
#| title: Total responses
#| icon: clipboard
#| color: "#007765"
responses |>
  nrow()
```

## Row {height=10%}

```{r geo-1}
#| title: Map of Lower 48 US Birthplaces
# R is excellent for handling, transforming, and displaying complex geographic data
# This uses the geocode() function from tidygeocoder to look up geographic info on cities & states using the Location IQ API
# Then it saves and converts it to a useable format, sf, which can be displayed using ggplot alongside a map of the US
# To run this yourself, you'll need to sign up with Location IQ and save their API key to your .Renviron
# > Uncomment and run the following line
# usethis::edit_r_environ()
# > Then add LOCATIONIQ_API_KEY = whatever_your_key_is, save, and restart R to proceed
geo_cities <- responses |>
  rename(
    city = in_what_city_were_you_born,
    state = for_us_attendees_in_what_state_were_you_born
  ) |>
  drop_na(state) |>
  mutate(country = "US") |>
  select(city, state, country) |>
  geocode(
    city = city,
    state = state,
    country = country,
    method = "iq"
  ) |>
  drop_na(long, lat) |>
  st_as_sf(
    coords = c("long", "lat"),
    crs = 4326
  )

# This imports the geospatial data needed to make a map of the United States
us_states <-
  states(progress_bar = FALSE) |>
  clean_names() |>
  select(name) |>
  shift_geometry()

# This creates the graph using two geom_sf objects
# The first one is a map of the lower 48 United States
# The second one is our "geo_cities" data that we created above
ggplot() +
  geom_sf(
    data = us_states,
    color = "grey90",
    fill = "grey90"
  ) +
  geom_sf(data = geo_cities) +
  theme_minimal() +
  theme(panel.grid = element_blank(), axis.text = element_blank())
```

```{r geo-2}
#| output: false
# What if we wanted to lump together our birthplace data into larger categories, to look for trends?
# R has many options for recategorizing data based on what criteria are met
# In this case, let's categorize birthplaces into regions

# First we need a list of which states are in which regions
# This is written in manually from https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States
# More advanced R usage could pull that table directly from Wikipedia!
region_northeast <- c(
  "Connecticut",
  "Maine",
  "Massachusetts",
  "New Hampshire",
  "Rhode Island",
  "Vermont",
  "New Jersey",
  "New York",
  "Pennsylvania"
)
region_midwest <- c(
  "Illinois",
  "Indiana",
  "Michigan",
  "Ohio",
  "Wisconsin",
  "Iowa",
  "Kansas",
  "Minnesota",
  "Missouri",
  "Nebraska",
  "North Dakota",
  "South Dakota"
)
region_south <- c(
  "Delaware",
  "District of Columbia",
  "Florida",
  "Georgia",
  "Maryland",
  "North Carolina",
  "South Carolina",
  "Virginia",
  "West Virginia",
  "Alabama",
  "Kentucky",
  "Mississippi",
  "Tennessee",
  "Arkansas",
  "Louisiana",
  "Oklahoma",
  "Texas"
)
region_west <- c(
  "Arizona",
  "Colorado",
  "Idaho",
  "Montana",
  "Nevada",
  "New Mexico",
  "Utah",
  "Wyoming",
  "Alaska",
  "California",
  "Hawaii",
  "Oregon",
  "Washington"
)

# One way to recategorize is using case_when, which can create a column based on a series of tests
# The test we're using here is %in%, which looks to see whether something is one of the items in a vector (the region_ objects defined above)
# If that test is true, then it fills in that column with whatever is after the tilde (~)
# If all tests pass false, then it puts in whatever is listed for .default at the end
responses |>
  rename(state = for_us_attendees_in_what_state_were_you_born) |>
  mutate(
    region = case_when(
      state %in% region_northeast ~ "Northeast",
      state %in% region_midwest ~ "Midwest",
      state %in% region_south ~ "South",
      state %in% region_west ~ "West",
      .default = NA
    )
  ) |>
  select(state, region)

# Another way to categorize is to use a join
# This is similar to XLOOKUP in Excel-- using a given column in one data table, it finds matches from a corresponding column in another table
# This is easiest to understand if we have all of the regions in a single table, so let's do that first
# What's happening here is each of the vectors above are being turned into tables with their appropriate regions labeled, and then bind_rows is used to attach them all together into one large table
regions_us <- bind_rows(
  region_northeast |>
    as_tibble() |>
    rename(state = value) |>
    mutate(region = "Northeast"),
  region_midwest |>
    as_tibble() |>
    rename(state = value) |>
    mutate(region = "Midwest"),
  region_south |>
    as_tibble() |>
    rename(state = value) |>
    mutate(region = "South"),
  region_west |> as_tibble() |> rename(state = value) |> mutate(region = "West")
) |>
  arrange(state)

# Now, instead of using case_when, we instead join responses & regions_us
# Because both have columns named "state", R knows to automatically fill in the appropriate region for each row
responses |>
  rename(state = for_us_attendees_in_what_state_were_you_born) |>
  left_join(regions_us) |>
  select(state, region)
```

```{r geo-3}
#| title: Lower 48 US Birth Regions
# Let's show the birth regions via a bar chart
# Changing the region into a factor like we did above, allows us to order the regions non-alphabetically
responses |>
  rename(state = for_us_attendees_in_what_state_were_you_born) |>
  left_join(regions_us) |>
  select(region) |>
  group_by(region) |>
  count() |>
  ungroup() |>
  drop_na() |>
  ggplot(aes(
    x = n,
    y = fct(region, levels = c("Northeast", "Midwest", "West", "South")),
    fill = region
  )) +
  geom_bar(stat = "identity") +
  theme_minimal(base_size = 20) +
  theme(
    panel.grid = element_blank(),
    legend.position = "none",
    axis.title = element_blank()
  )
```

## Row {height=10%}

```{r gender}
#| title: Gender (categorized from raw data)
#| fig-height: 2
# While the things we've shown above could be done in Excel, Google Sheets, or similar tools
# Other R packages provide extremely powerful functionality that wouldn't be possible with other platforms
# One example is gendercoder, which is designed to handle free-response gender data
# You can read more about the package here: https://github.com/ropensci/gendercoder
responses |>
  rename(gender = what_is_your_gender) |>
  mutate(gender_coded = recode_gender(gender)) |>
  group_by(gender_coded) |>
  count() |>
  ungroup() |>
  ggplot(aes(x = "", y = n, fill = gender_coded)) +
  geom_bar(stat = "identity") +
  coord_polar("y", start = 0) +
  theme_void()

# A pie chart is made from the data above by creating a stacked bar chart, and then changing to polar coordinates
# Commenting out the coord_polar() line above would change it back
```
